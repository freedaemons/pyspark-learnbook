{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "import pandas as pd\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_train_csv = os.path.join('input', 'titanic', 'train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.load(titanic_train_csv, format='csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| null|       S|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In R's dplyr package, Hadley Wickham defined the 5 basic verbs - select, filter, mutate, summarize, and arrange. Here are the equivalents for Spark Dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+\n",
      "|PassengerId|Survived|\n",
      "+-----------+--------+\n",
      "|          1|       0|\n",
      "|          2|       1|\n",
      "|          3|       1|\n",
      "|          4|       1|\n",
      "|          5|       0|\n",
      "+-----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(['PassengerId', 'Survived']).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+--------+\n",
      "|                Name|   Sex|Survived|\n",
      "+--------------------+------+--------+\n",
      "|Cumings, Mrs. Joh...|female|       1|\n",
      "|Heikkinen, Miss. ...|female|       1|\n",
      "|Futrelle, Mrs. Ja...|female|       1|\n",
      "|Johnson, Mrs. Osc...|female|       1|\n",
      "|Nasser, Mrs. Nich...|female|       1|\n",
      "+--------------------+------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(['Name', 'Sex', 'Survived']).filter(df.Sex == 'female').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+--------+\n",
      "|                Name|   Sex|Survived|\n",
      "+--------------------+------+--------+\n",
      "|Vestrom, Miss. Hu...|female|       0|\n",
      "|Vander Planke, Mr...|female|       0|\n",
      "|Palsson, Miss. To...|female|       0|\n",
      "|Vander Planke, Mi...|female|       0|\n",
      "|Ahlin, Mrs. Johan...|female|       0|\n",
      "+--------------------+------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Filter also works with SQL expressions\n",
    "df.select(['Name', 'Sex', 'Survived']).filter(\"Sex='female' AND Survived=0\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+---------+\n",
      "|                Name|   Sex|Last Name|\n",
      "+--------------------+------+---------+\n",
      "|Braund, Mr. Owen ...|  male|   Braund|\n",
      "|Cumings, Mrs. Joh...|female|  Cumings|\n",
      "|Heikkinen, Miss. ...|female|Heikkinen|\n",
      "|Futrelle, Mrs. Ja...|female| Futrelle|\n",
      "|Allen, Mr. Willia...|  male|    Allen|\n",
      "+--------------------+------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Mutate, i.e. adding new columns, seems to only be doable by chaining .withColumn()\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def getLastName(fullName):\n",
    "    return fullName.split(',')[0]\n",
    "\n",
    "getLastName_udf = udf(lambda x: getLastName(x), StringType())\n",
    "\n",
    "df.select(['Name', 'Sex']).withColumn('Last Name', getLastName_udf(df.Name)).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-----+------------------+------------------+\n",
      "|   Sex|Pclass|count|       average_age|        total_fare|\n",
      "+------+------+-----+------------------+------------------+\n",
      "|  male|     3|  347|26.507588932806325| 4393.586500000005|\n",
      "|female|     3|  144|             21.75|2321.1086000000005|\n",
      "|female|     1|   94| 34.61176470588235| 9975.824999999999|\n",
      "|female|     2|   76|28.722972972972972|         1669.7292|\n",
      "|  male|     2|  108| 30.74070707070707|         2132.1125|\n",
      "|  male|     1|  122| 41.28138613861386| 8201.587500000001|\n",
      "+------+------+-----+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Summarize\n",
    "gdf = df.groupby('Sex', 'Pclass').agg({\n",
    "    '*': 'count',\n",
    "    'Age': 'avg',\n",
    "    'Fare': 'sum'\n",
    "}).toDF('Sex', 'Pclass', 'count', 'average_age', 'total_fare')\n",
    "gdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-----+------------------+------------------+\n",
      "|   Sex|Pclass|count|       average_age|        total_fare|\n",
      "+------+------+-----+------------------+------------------+\n",
      "|  male|     1|  122| 41.28138613861386| 8201.587500000001|\n",
      "|  male|     2|  108| 30.74070707070707|         2132.1125|\n",
      "|  male|     3|  347|26.507588932806325| 4393.586500000005|\n",
      "|female|     1|   94| 34.61176470588235| 9975.824999999999|\n",
      "|female|     2|   76|28.722972972972972|         1669.7292|\n",
      "|female|     3|  144|             21.75|2321.1086000000005|\n",
      "+------+------+-----+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Arrange\n",
    "gdf.orderBy(['Sex', 'Pclass'], ascending=[False, True]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+--------------------+------+----+\n",
      "|PassengerId|Pclass|                Name|   Sex| Age|\n",
      "+-----------+------+--------------------+------+----+\n",
      "|          1|     3|Braund, Mr. Owen ...|  male|22.0|\n",
      "|          2|     1|Cumings, Mrs. Joh...|female|38.0|\n",
      "|          3|     3|Heikkinen, Miss. ...|female|26.0|\n",
      "|          4|     1|Futrelle, Mrs. Ja...|female|35.0|\n",
      "|          5|     3|Allen, Mr. Willia...|  male|35.0|\n",
      "+-----------+------+--------------------+------+----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------+--------+\n",
      "|PassengerId|Survived|\n",
      "+-----------+--------+\n",
      "|          1|       0|\n",
      "|          2|       1|\n",
      "|          3|       1|\n",
      "|          4|       1|\n",
      "|          5|       0|\n",
      "+-----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Joins\n",
    "df1 = df.select(['PassengerId', 'Pclass', 'Name', 'Sex', 'Age'])\n",
    "df2 = df.select(['PassengerId', 'Survived'])\n",
    "df1.show(5)\n",
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+--------------------+------+----+--------+\n",
      "|PassengerId|Pclass|                Name|   Sex| Age|Survived|\n",
      "+-----------+------+--------------------+------+----+--------+\n",
      "|          1|     3|Braund, Mr. Owen ...|  male|22.0|       0|\n",
      "|          2|     1|Cumings, Mrs. Joh...|female|38.0|       1|\n",
      "|          3|     3|Heikkinen, Miss. ...|female|26.0|       1|\n",
      "|          4|     1|Futrelle, Mrs. Ja...|female|35.0|       1|\n",
      "|          5|     3|Allen, Mr. Willia...|  male|35.0|       0|\n",
      "+-----------+------+--------------------+------+----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.join(df2, ['PassengerId']).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+--------------------+------+-----------+--------+\n",
      "|PassengerId|Pclass|                Name|   Sex|PassengerId|Survived|\n",
      "+-----------+------+--------------------+------+-----------+--------+\n",
      "|          1|     3|Braund, Mr. Owen ...|  male|          1|       0|\n",
      "|          1|     3|Braund, Mr. Owen ...|  male|          2|       1|\n",
      "|          1|     3|Braund, Mr. Owen ...|  male|          3|       1|\n",
      "|          1|     3|Braund, Mr. Owen ...|  male|          4|       1|\n",
      "|          1|     3|Braund, Mr. Owen ...|  male|          5|       0|\n",
      "|          2|     1|Cumings, Mrs. Joh...|female|          2|       1|\n",
      "|          2|     1|Cumings, Mrs. Joh...|female|          3|       1|\n",
      "|          2|     1|Cumings, Mrs. Joh...|female|          4|       1|\n",
      "|          2|     1|Cumings, Mrs. Joh...|female|          5|       0|\n",
      "|          3|     3|Heikkinen, Miss. ...|female|          3|       1|\n",
      "|          3|     3|Heikkinen, Miss. ...|female|          4|       1|\n",
      "|          3|     3|Heikkinen, Miss. ...|female|          5|       0|\n",
      "|          4|     1|Futrelle, Mrs. Ja...|female|          4|       1|\n",
      "|          4|     1|Futrelle, Mrs. Ja...|female|          5|       0|\n",
      "|          5|     3|Allen, Mr. Willia...|  male|          5|       0|\n",
      "+-----------+------+--------------------+------+-----------+--------+\n",
      "\n",
      "Wall time: 240 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Non-equi joins, i.e. joining not by column name, but by values in column\n",
    "#Slow due to skewed data, but at least it can be done in Spark, unlike Hive\n",
    "\n",
    "\"\"\"\n",
    "In this case, because the dataframes I am joining have the same origin, it's necessary to give them\n",
    "aliases so that Spark doesn't think that I'm performing a trivial crossjoin.\n",
    "Ordinarily, between dataframes of separate lineage, aliases are unnecessary,\n",
    "and the following syntax would suffice:\n",
    "\"\"\"\n",
    "\n",
    "#df4.join(df5, df4.PassengerId <= df5.PassengerId).show()b\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df3 = df.select(['PassengerId', 'Pclass','Name', 'Sex', 'Survived']).filter(\"PassengerId<=5\")\n",
    "df4 = df3.select(['PassengerId', 'Pclass', 'Name', 'Sex']).alias('dataframe1')\n",
    "df5 = df3.select(['PassengerId', 'Survived']).alias('dataframe2')\n",
    "\n",
    "df4.join(df5, on=col('dataframe1.PassengerId') <= col('dataframe2.PassengerId')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+--------------------+------+--------+\n",
      "|PassengerId|Pclass|                Name|   Sex|Survived|\n",
      "+-----------+------+--------------------+------+--------+\n",
      "|          1|     3|Braund, Mr. Owen ...|  male|       0|\n",
      "|          2|     1|Cumings, Mrs. Joh...|female|       1|\n",
      "|          3|     3|Heikkinen, Miss. ...|female|       1|\n",
      "|          4|     1|Futrelle, Mrs. Ja...|female|       1|\n",
      "|          5|     3|Allen, Mr. Willia...|  male|       0|\n",
      "|          1|     3|Braund, Mr. Owen ...|  male|       0|\n",
      "|          2|     1|Cumings, Mrs. Joh...|female|       1|\n",
      "|          3|     3|Heikkinen, Miss. ...|female|       1|\n",
      "|          4|     1|Futrelle, Mrs. Ja...|female|       1|\n",
      "|          5|     3|Allen, Mr. Willia...|  male|       0|\n",
      "+-----------+------+--------------------+------+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nIn loops we sometimes chain union() objects, e.g. to use as a stack, which may cause performance issues\\nrepartition() and checkpoint() may help in solving this problem\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Union, i.e. append dataframes. Columns must be shared.\n",
    "df3.union(df3).show()\n",
    "\n",
    "\"\"\"\n",
    "In loops we sometimes chain union() objects, e.g. to use as a stack, which may cause performance issues\n",
    "repartition() and checkpoint() may help in solving this problem\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write to file\n",
    "df2.write.csv(os.path.join('output', 'titanic', 'train_targets'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+--------------------+------+----+--------+\n",
      "|PassengerId|Pclass|                Name|   Sex| Age|Survived|\n",
      "+-----------+------+--------------------+------+----+--------+\n",
      "|          1|     3|Braund, Mr. Owen ...|  male|22.0|       0|\n",
      "|          2|     1|Cumings, Mrs. Joh...|female|38.0|       1|\n",
      "|          3|     3|Heikkinen, Miss. ...|female|26.0|       1|\n",
      "|          4|     1|Futrelle, Mrs. Ja...|female|35.0|       1|\n",
      "|          5|     3|Allen, Mr. Willia...|  male|35.0|       0|\n",
      "|          6|     3|    Moran, Mr. James|  male|null|       0|\n",
      "|          7|     1|McCarthy, Mr. Tim...|  male|54.0|       0|\n",
      "|          8|     3|Palsson, Master. ...|  male| 2.0|       0|\n",
      "|          9|     3|Johnson, Mrs. Osc...|female|27.0|       1|\n",
      "|         10|     2|Nasser, Mrs. Nich...|female|14.0|       1|\n",
      "|         11|     3|Sandstrom, Miss. ...|female| 4.0|       1|\n",
      "|         12|     1|Bonnell, Miss. El...|female|58.0|       1|\n",
      "|         13|     3|Saundercock, Mr. ...|  male|20.0|       0|\n",
      "|         14|     3|Andersson, Mr. An...|  male|39.0|       0|\n",
      "|         15|     3|Vestrom, Miss. Hu...|female|14.0|       0|\n",
      "|         16|     2|Hewlett, Mrs. (Ma...|female|55.0|       1|\n",
      "|         17|     3|Rice, Master. Eugene|  male| 2.0|       0|\n",
      "|         18|     2|Williams, Mr. Cha...|  male|null|       1|\n",
      "|         19|     3|Vander Planke, Mr...|female|31.0|       0|\n",
      "|         20|     3|Masselmani, Mrs. ...|female|null|       1|\n",
      "+-----------+------+--------------------+------+----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Almost everything done above can be done by writing Hive SQL queries in spark.sql()\n",
    "The queries would act on tables, not dataframes, so to illustrate we'll convert the dataframes to \n",
    "temp tables\n",
    "\"\"\"\n",
    "\n",
    "df1.createOrReplaceTempView('df1_temp')\n",
    "df2.createOrReplaceTempView('df2_temp')\n",
    "\n",
    "query='''\n",
    "    select\n",
    "        a.PassengerId,\n",
    "        a.Pclass,\n",
    "        a.Name,\n",
    "        a.Sex,\n",
    "        a.Age,\n",
    "        b.Survived\n",
    "    from df1_temp a\n",
    "    join df2_temp b\n",
    "        on a.PassengerId = b.PassengerId\n",
    "'''\n",
    "dfj = spark.sql(query)\n",
    "dfj.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) Project [PassengerId#10, Pclass#12, Name#13, Sex#14, Age#15, Survived#424]\n",
      "+- *(2) BroadcastHashJoin [PassengerId#10], [PassengerId#423], Inner, BuildRight\n",
      "   :- *(2) Project [PassengerId#10, Pclass#12, Name#13, Sex#14, Age#15]\n",
      "   :  +- *(2) Filter isnotnull(PassengerId#10)\n",
      "   :     +- *(2) FileScan csv [PassengerId#10,Pclass#12,Name#13,Sex#14,Age#15] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/friedemann.ang/Documents/repos/pyspark-learnbook/input/titanic/t..., PartitionFilters: [], PushedFilters: [IsNotNull(PassengerId)], ReadSchema: struct<PassengerId:int,Pclass:int,Name:string,Sex:string,Age:double>\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n",
      "      +- *(1) Project [PassengerId#423, Survived#424]\n",
      "         +- *(1) Filter isnotnull(PassengerId#423)\n",
      "            +- *(1) FileScan csv [PassengerId#423,Survived#424] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/friedemann.ang/Documents/repos/pyspark-learnbook/input/titanic/t..., PartitionFilters: [], PushedFilters: [IsNotNull(PassengerId)], ReadSchema: struct<PassengerId:int,Survived:int>\n"
     ]
    }
   ],
   "source": [
    "dfj.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two types of operations in Spark: transformations and actions. \n",
    "\n",
    ".explain() lays out the plan to transform some input to obtain some further state, but it isn't executed until some action is called, e.g. show(), count(), saveAsTable() etc.\n",
    "\n",
    "A good execution plan means good performance, so examining the plan using explain() is a good way to tune the performance of Spark jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StorageLevel(True, True, False, True, 1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Caching\n",
    "\n",
    "Cache a dataframe when it is used multiple times in a script.\n",
    "This can reduce the number of times an execution plan is carried out to obtain the dataframe, \n",
    "and so improve performance.\n",
    "\"\"\"\n",
    "#cache() only works after at least one action has been called on a dataframe\n",
    "df1.cache()\n",
    "#to see if a dataframe is cachhed, check the storageLevel property\n",
    "df1.storageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StorageLevel(False, False, False, False, 1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to uncache a dataframe, use unpersist()\n",
    "df1.unpersist()\n",
    "df1.storageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Checkpointing\n",
    "\n",
    "checkpoint()  truncates an execution plan and caches the checkpointed dataframe to a temporary\n",
    "location on the disk. \n",
    "\n",
    "It's recommended that you cache before checkpointing so that Spark doesn't have to read the dataframe\n",
    "from disk again after it's checkpointed.\n",
    "\"\"\"\n",
    "\n",
    "#To use checkpoint(), it's necessary to specify the temporary file location to save the dataframe to, \n",
    "#by accessing the sparkContext object from SparkSession\n",
    "\n",
    "sc.setCheckpointDir('/checkpointdir') #save to c:/checkpointdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(3) Project [PassengerId#10, Pclass#12, Name#13, Sex#14, Age#15, Pclass#500, Name#501, Sex#502, Age#503, Pclass#521, Name#522, Sex#523, Age#524]\n",
      "+- *(3) BroadcastHashJoin [PassengerId#10], [PassengerId#519], Inner, BuildRight\n",
      "   :- *(3) Project [PassengerId#10, Pclass#12, Name#13, Sex#14, Age#15, Pclass#500, Name#501, Sex#502, Age#503]\n",
      "   :  +- *(3) BroadcastHashJoin [PassengerId#10], [PassengerId#498], Inner, BuildRight\n",
      "   :     :- *(3) Project [PassengerId#10, Pclass#12, Name#13, Sex#14, Age#15]\n",
      "   :     :  +- *(3) Filter isnotnull(PassengerId#10)\n",
      "   :     :     +- *(3) FileScan csv [PassengerId#10,Pclass#12,Name#13,Sex#14,Age#15] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/friedemann.ang/Documents/repos/pyspark-learnbook/input/titanic/t..., PartitionFilters: [], PushedFilters: [IsNotNull(PassengerId)], ReadSchema: struct<PassengerId:int,Pclass:int,Name:string,Sex:string,Age:double>\n",
      "   :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n",
      "   :        +- *(1) Project [PassengerId#498, Pclass#500, Name#501, Sex#502, Age#503]\n",
      "   :           +- *(1) Filter isnotnull(PassengerId#498)\n",
      "   :              +- *(1) FileScan csv [PassengerId#498,Pclass#500,Name#501,Sex#502,Age#503] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/friedemann.ang/Documents/repos/pyspark-learnbook/input/titanic/t..., PartitionFilters: [], PushedFilters: [IsNotNull(PassengerId)], ReadSchema: struct<PassengerId:int,Pclass:int,Name:string,Sex:string,Age:double>\n",
      "   +- ReusedExchange [PassengerId#519, Pclass#521, Name#522, Sex#523, Age#524], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n"
     ]
    }
   ],
   "source": [
    "dfc = df1.join(df1, ['PassengerId'])\n",
    "dfc.join(df1, ['PassengerId']).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) Project [PassengerId#10, Pclass#12, Name#13, Sex#14, Age#15, Pclass#546, Name#547, Sex#548, Age#549, Pclass#587, Name#588, Sex#589, Age#590]\n",
      "+- *(2) BroadcastHashJoin [PassengerId#10], [PassengerId#585], Inner, BuildRight\n",
      "   :- *(2) Filter isnotnull(PassengerId#10)\n",
      "   :  +- Scan ExistingRDD[PassengerId#10,Pclass#12,Name#13,Sex#14,Age#15,Pclass#546,Name#547,Sex#548,Age#549]\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n",
      "      +- *(1) Project [PassengerId#585, Pclass#587, Name#588, Sex#589, Age#590]\n",
      "         +- *(1) Filter isnotnull(PassengerId#585)\n",
      "            +- *(1) FileScan csv [PassengerId#585,Pclass#587,Name#588,Sex#589,Age#590] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/friedemann.ang/Documents/repos/pyspark-learnbook/input/titanic/t..., PartitionFilters: [], PushedFilters: [IsNotNull(PassengerId)], ReadSchema: struct<PassengerId:int,Pclass:int,Name:string,Sex:string,Age:double>\n"
     ]
    }
   ],
   "source": [
    "#Checkpointing after the first join would truncate the plan\n",
    "dfc2 = df1.join(df1, ['PassengerId']).checkpoint()\n",
    "dfc2.join(df1, ['PassengerId']).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Partitioning\n",
    "\n",
    "Having too many partitions can cause inefficiencies. See: Top 5 Mistakes when writing Spark applications\n",
    "For Spark, rule of thumb is to have around 128MB per partition.\n",
    "\"\"\"\n",
    "\n",
    "df1.rdd.getNumPartitions()\n",
    "#df.repartition(numPartitions)\n",
    "#df.coalaesce(numPartitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like in this version of Spark, the automatic partitioning is more accurate than when the guide on https://changhsinlee.com/pyspark-dataframe-basics/ was written for Spark 2.1\n",
    "\n",
    "If there are too many/too few partitions, we can repartition using df.repartition(numPartitions)\n",
    "If there are close to 2000 partitions, it's recommended to bump that number up to slightly higher than 2000, because Spark uses a different, more compressed/efficient data structure for bookkeeping during block shuffling when there are more than 2000 partitions.\n",
    "\n",
    "If we know for certain we're reducing the number of partitions, we can use coalesce() instead of repartition to avoid a full reshuffle, by retaining the upcoming number of partitions and distributing  the data from the extra partitions onto them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Supposedly gets the number of nodes in the cluster, including head and worker nodes\n",
    "sc._jsc.sc().getExecutorMemoryStatus().keySet().size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
